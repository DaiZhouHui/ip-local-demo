#!/usr/bin/env python3
"""
build_database_three_tiers.py - ä¸‰çº§ç²¾åº¦å·®å¼‚åŒ–æ„å»ºå™¨
å±‚çº§1: æ ¸å¿ƒ8å›½/åœ°åŒº - é«˜ç²¾åº¦ (æ¸©å’Œåˆå¹¶)
å±‚çº§2: æ¬¡è¦10å›½ - ä¸­ç­‰ç²¾åº¦ (é€‚åº¦åˆå¹¶)
å±‚çº§3: å…¶ä½™æ‰€æœ‰ - ä½ç²¾åº¦ (è¶…çº§åˆå¹¶ä¸ºâ€˜ZZâ€™)
"""

import csv, json, os, shutil, sys
from urllib.request import urlopen
from ipaddress import ip_network
from datetime import datetime, timezone
MAXMIND_LICENSE_KEY = os.environ.get("MAXMIND_LICENSE_KEY", "")

# æ·»åŠ æ£€æŸ¥ï¼Œå¦‚æœå¯†é’¥ä¸ºç©ºåˆ™æŠ¥é”™
if not MAXMIND_LICENSE_KEY:
    print("âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°MaxMindè®¸å¯è¯å¯†é’¥ã€‚")
    print("   è¯·è®¾ç½® MAXMIND_LICENSE_KEY ç¯å¢ƒå˜é‡ã€‚")
    print("   æœ¬åœ°æµ‹è¯•ï¼šåœ¨.envæ–‡ä»¶ä¸­è®¾ç½®ï¼Œæˆ–è¿è¡Œå‰æ‰§è¡Œ export MAXMIND_LICENSE_KEY=ä½ çš„å¯†é’¥")
    print("   GitHub Actionsï¼šå·²åœ¨ä»“åº“Secretsä¸­è®¾ç½®")
    sys.exit(1)

OUTPUT_JSON = "database.json"  # å›ºå®šæ–‡ä»¶åï¼Œä¸å‰ç«¯åŒ¹é…
TEMP_DIR = "temp_tiered_data"

# ====== ä¸‰çº§ç²¾åº¦é…ç½® ======
# ç¬¬ä¸€å±‚çº§ï¼šæ ¸å¿ƒ8å›½/åœ°åŒº (é«˜ç²¾åº¦ï¼Œæ¸©å’Œåˆå¹¶)
TIER1_COUNTRIES = {
    'CN',  # ä¸­å›½
    'HK',  # ä¸­å›½é¦™æ¸¯
    'JP',  # æ—¥æœ¬
    'KR',  # éŸ©å›½
    'US',  # ç¾å›½
    'AU',  # æ¾³å¤§åˆ©äºš
    'NZ',  # æ–°è¥¿å…°
    'SG',  # æ–°åŠ å¡
}

# ç¬¬äºŒå±‚çº§ï¼šæ¬¡è¦10å›½ (ä¸­ç­‰ç²¾åº¦ï¼Œé€‚åº¦æ¿€è¿›åˆå¹¶)
TIER2_COUNTRIES = {
    'DE',  # å¾·å›½
    'GB',  # è‹±å›½
    'FR',  # æ³•å›½
    'RU',  # ä¿„ç½—æ–¯
    'IN',  # å°åº¦
    'CA',  # åŠ æ‹¿å¤§
    'IT',  # æ„å¤§åˆ©
    'NL',  # è·å…°
    'TW',  # ä¸­å›½å°æ¹¾
    # æ³¨æ„ï¼šå·²ç§»é™¤ 'BR' (å·´è¥¿)
}

OTHER_COUNTRY_CODE = 'ZZ'  # ç¬¬ä¸‰å±‚çº§ï¼šå…¶ä½™æ‰€æœ‰å›½å®¶

def download_and_extract():
    """ä¸‹è½½å¹¶æå–æ•°æ®"""
    print("[1] ä¸‹è½½æ•°æ®...")
    os.makedirs(TEMP_DIR, exist_ok=True)
    download_url = f"https://download.maxmind.com/app/geoip_download?edition_id=GeoLite2-Country-CSV&license_key={MAXMIND_LICENSE_KEY}&suffix=zip"
    zip_path = os.path.join(TEMP_DIR, "source.zip")
    
    try:
        with urlopen(download_url) as res, open(zip_path, 'wb') as f:
            shutil.copyfileobj(res, f)
        shutil.unpack_archive(zip_path, TEMP_DIR)
        
        for item in os.listdir(TEMP_DIR):
            if item.startswith("GeoLite2-Country-CSV_"):
                csv_dir = os.path.join(TEMP_DIR, item)
                return (
                    os.path.join(csv_dir, "GeoLite2-Country-Blocks-IPv4.csv"),
                    os.path.join(csv_dir, "GeoLite2-Country-Locations-zh-CN.csv")
                )
    except Exception as e:
        print(f"    âŒ å¤±è´¥: {e}")
        raise Exception(f"ä¸‹è½½æˆ–è§£å‹å¤±è´¥: {e}")

def tiered_merge_ranges(ranges, tier_level):
    """
    æ ¹æ®å±‚çº§é‡‡ç”¨ä¸åŒçš„åˆå¹¶ç­–ç•¥
    tier_level: 1=é«˜ç²¾åº¦, 2=ä¸­ç²¾åº¦, 3=ä½ç²¾åº¦
    """
    if not ranges:
        return []
    
    ranges.sort()
    merged = []
    cs, ce = ranges[0]
    
    for s, e in ranges[1:]:
        if tier_level == 1:
            # å±‚çº§1ï¼šé«˜ç²¾åº¦ï¼Œåªåˆå¹¶ç›´æ¥ç›¸é‚»çš„
            merge_threshold = 1
        elif tier_level == 2:
            # å±‚çº§2ï¼šä¸­ç²¾åº¦ï¼Œå…è®¸å°é—´éš™ (çº¦1024ä¸ªCç±»ç½‘æ®µ)
            merge_threshold = 262144
        else:
            # å±‚çº§3ï¼šä½ç²¾åº¦ï¼Œå…è®¸è¶…å¤§é—´éš™ (çº¦65536ä¸ªCç±»ç½‘æ®µ)
            merge_threshold = 16777216
        
        if s - ce <= merge_threshold:
            if e > ce:
                ce = e
        else:
            merged.append((cs, ce))
            cs, ce = s, e
    
    merged.append((cs, ce))
    return merged

def main():
    print("=" * 60)
    print("ä¸‰çº§ç²¾åº¦å·®å¼‚åŒ–IPæ•°æ®åº“æ„å»ºå™¨")
    print("=" * 60)
    print(f"ç¬¬ä¸€å±‚çº§ (æ ¸å¿ƒ8å›½): {sorted(TIER1_COUNTRIES)}")
    print(f"ç¬¬äºŒå±‚çº§ (æ¬¡è¦10å›½): {sorted(TIER2_COUNTRIES)}")
    print(f"ç¬¬ä¸‰å±‚çº§ (å…¶ä½™æ‰€æœ‰): '{OTHER_COUNTRY_CODE}'")
    print("=" * 60)
    
    if "YOUR_MAXMIND" in MAXMIND_LICENSE_KEY:
        print("âŒ é”™è¯¯ï¼šè¯·å…ˆé…ç½®ä½ çš„MaxMindè®¸å¯è¯å¯†é’¥")
        sys.exit(1)
    
    # æ¸…ç†å¹¶å¼€å§‹
    if os.path.exists(TEMP_DIR):
        shutil.rmtree(TEMP_DIR)
    
    try:
        # 1. ä¸‹è½½
        # type: ignore
        blocks_file, locations_file = download_and_extract()
        
        # 2. åŠ è½½å›½å®¶æ˜ å°„
        print("\n[2] åŠ è½½å›½å®¶æ˜ å°„...")
        country_map = {}
        try:
            with open(locations_file, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    country_map[row['geoname_id']] = row['country_iso_code']
        except:
            eng_file = locations_file.replace('zh-CN', 'en')
            with open(eng_file, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    country_map[row['geoname_id']] = row['country_iso_code']
        
        # 3. æŒ‰ä¸‰çº§åˆ†ç±»æ”¶é›†æ•°æ®
        print("[3] åˆ†ç±»æ”¶é›†IPæ®µæ•°æ®...")
        tier1_data = {c: [] for c in TIER1_COUNTRIES}
        tier2_data = {c: [] for c in TIER2_COUNTRIES}
        tier3_data = []  # å…¶ä»–å›½å®¶
        
        with open(blocks_file, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for i, row in enumerate(reader):
                if i % 200000 == 0 and i > 0:
                    print(f"    å·²å¤„ç† {i:,} è¡Œ...")
                
                geoname_id = row.get('registered_country_geoname_id') or row.get('geoname_id')
                country = country_map.get(geoname_id, '')  # å¦‚æœæ‰¾ä¸åˆ°ï¼Œé»˜è®¤ä¸ºç©ºå­—ç¬¦ä¸²
                if not country:
                    continue
                
                try:
                    net = ip_network(row['network'].strip())
                    start, end = int(net.network_address), int(net.broadcast_address)
                    
                    if country in TIER1_COUNTRIES:
                        tier1_data[country].append((start, end))
                    elif country in TIER2_COUNTRIES:
                        tier2_data[country].append((start, end))
                    else:
                        tier3_data.append((start, end))
                except:
                    continue
        
        # 4. ä¸‰çº§å·®å¼‚åŒ–åˆå¹¶
        print("\n[4] æ‰§è¡Œä¸‰çº§å·®å¼‚åŒ–åˆå¹¶...")
        all_entries = []
        
        # 4.1 ç¬¬ä¸€å±‚çº§ï¼šæ ¸å¿ƒ8å›½ (é«˜ç²¾åº¦)
        print("    ç¬¬ä¸€å±‚çº§ (æ ¸å¿ƒ8å›½ - é«˜ç²¾åº¦):")
        for country in sorted(TIER1_COUNTRIES):
            ranges = tier1_data[country]
            if not ranges:
                continue
            merged = tiered_merge_ranges(ranges, tier_level=1)
            for s, e in merged:
                all_entries.append([s, e, country])
            print(f"        {country}: {len(ranges):,} -> {len(merged):,} åŒºé—´")
        
        # 4.2 ç¬¬äºŒå±‚çº§ï¼šæ¬¡è¦10å›½ (ä¸­ç²¾åº¦)
        print("    ç¬¬äºŒå±‚çº§ (æ¬¡è¦10å›½ - ä¸­ç²¾åº¦):")
        for country in sorted(TIER2_COUNTRIES):
            ranges = tier2_data[country]
            if not ranges:
                continue
            merged = tiered_merge_ranges(ranges, tier_level=2)
            for s, e in merged:
                all_entries.append([s, e, country])
            print(f"        {country}: {len(ranges):,} -> {len(merged):,} åŒºé—´")
        
        # 4.3 ç¬¬ä¸‰å±‚çº§ï¼šå…¶ä½™æ‰€æœ‰ (ä½ç²¾åº¦)
        print("    ç¬¬ä¸‰å±‚çº§ (å…¶ä½™æ‰€æœ‰ - ä½ç²¾åº¦):")
        if tier3_data:
            merged = tiered_merge_ranges(tier3_data, tier_level=3)
            for s, e in merged:
                all_entries.append([s, e, OTHER_COUNTRY_CODE])
            print(f"        å…¶ä½™å›½å®¶: {len(tier3_data):,} -> {len(merged):,} åŒºé—´")
            print(f"        æ ‡è®°ä¸º: '{OTHER_COUNTRY_CODE}'")
        
        # 5. æ’åºå¹¶ä¿å­˜
        print("\n[5] ä¿å­˜ä¸ºä¼˜åŒ–JSON...")
        all_entries.sort(key=lambda x: x[0])
        
        result = {
            "meta": {
                "version": "three-tier-v1",
                "tier1": sorted(list(TIER1_COUNTRIES)),
                "tier2": sorted(list(TIER2_COUNTRIES)),
                "other": OTHER_COUNTRY_CODE,
                "generated": datetime.now(timezone.utc).strftime("%Y-%m-%d"),
                "totalRanges": len(all_entries)
            },
            "data": all_entries
        }
        
        with open(OUTPUT_JSON, 'w', encoding='utf-8') as f:
            json.dump(result, f, separators=(',', ':'))
        
        size = os.path.getsize(OUTPUT_JSON)
        size_kb = size / 1024
        size_mb = size_kb / 1024
        
        print(f"    æœ€ç»ˆåŒºé—´æ€»æ•°: {len(all_entries):,}")
        print(f"    æœ€ç»ˆæ–‡ä»¶å¤§å°: {size_mb:.2f} MB ({size_kb:.1f} KB)")
        
        # 6. é¢„æœŸæ•ˆæœåˆ†æ
        print("\n" + "=" * 60)
        print("âœ… ä¸‰çº§ç²¾åº¦æ•°æ®åº“æ„å»ºå®Œæˆï¼")
        print("=" * 60)
        
        # åŸºäºç¾å›½æ•°æ®å¤§å¹…å‡å°‘çš„é¢„æœŸ
        expected_savings = (84000 - len(all_entries)) / 84000 * 100
        print(f"\nğŸ“Š ä¼˜åŒ–æ•ˆæœ:")
        print(f"    â€¢ å¯¹æ¯”ä¹‹å‰84,300ä¸ªåŒºé—´ï¼Œé¢„è®¡å‡å°‘ {expected_savings:.1f}%")
        
        if size_mb > 1.2:
            print(f"\nâš ï¸  æ–‡ä»¶ä» >1.2MBï¼Œå¯è€ƒè™‘:")
            print(f"   1. å°†ç¬¬äºŒå±‚çº§å›½å®¶ç§»å…¥ç¬¬ä¸‰å±‚çº§ (æ”¹ä¸º'ZZ')")
            print(f"   2. åœ¨ tiered_merge_ranges() ä¸­è°ƒæ•´åˆå¹¶é˜ˆå€¼")
        elif size_mb > 0.8:
            print(f"\nğŸ“ˆ å¤§å°é€‚ä¸­ ({size_mb:.2f}MB)ï¼Œé€‚åˆå¼‚æ­¥åŠ è½½")
        else:
            print(f"\nâœ¨ ä¼˜åŒ–å‡ºè‰²ï¼æ–‡ä»¶ < 0.8MB")
        
        print(f"\nğŸ’¡ å‰ç«¯åŠ è½½æç¤º:")
        print(f"   æ–‡ä»¶å°†å¼‚æ­¥åŠ è½½ï¼Œä¸å½±å“é¡µé¢åˆå§‹æ˜¾ç¤º")
        
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        sys.exit(1)
        import traceback
        traceback.print_exc()
    finally:
        if os.path.exists(TEMP_DIR):
            shutil.rmtree(TEMP_DIR)
        print("=" * 60)

if __name__ == "__main__":
    main()